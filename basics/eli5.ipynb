{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](../images/architecture.png)\n",
    "\n",
    "In this example, we'll put together a complete, end to end flow for evaluating a simple AI Agent. We'll be utilizing the Explain Like I'm 5 (ELI5) Chatbot defined in ```eli5.py```.\n",
    "\n",
    "Let's define an simple agent development flow:\n",
    "1. User feedback is provided in your app for all AI interactions This feedback is sent to your team to repro and debug. \n",
    "    * For our demo, we'll have feedback be surfaced not just in LangSmith, but sent to a Slack channel.\n",
    "2. You replay the LLM call with varying inputs, prompts, or models to experiment and optimize the AI interaction.\n",
    "3. You codify your learnings from the user feedback into an evaluation with an LLM-as-a-judge evaluator.\n",
    "    * This allows you to easily check for regressions in the future for this particular type of feedback..\n",
    "4. Evaluations are run after each change to your AI Agent, and results are reported to your team.\n",
    "    * For our demo, we'll again use Slack to surface eval results.\n",
    "\n",
    "We'll create each step in this exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our environment variables from our .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "# Loads the following env variables\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5\"\n",
    "# LANGSMITH_API_KEY=\"<redacted>\"\n",
    "# OPENAI_API_KEY=\"<redacted>\"\n",
    "# TAVILY_API_KEY=\"<redacted>\"\n",
    "\n",
    "# SLACK_TOKEN=\"<redacted>\"\n",
    "# SLACK_CHANNEL=\"<redacted>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up our LangSmith client to use the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "ls_project = os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up a Slack client to send messages programmatically to Slack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from slack_sdk import WebClient\n",
    "\n",
    "slack_token = os.getenv(\"SLACK_TOKEN\")\n",
    "channel_id = os.getenv(\"SLACK_CHANNEL\")\n",
    "\n",
    "slack_client = WebClient(token=slack_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gathering Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function that can be called by the application containing your AI agent. You'll call this function when a user provides feedback to the ELI5 agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_user_feedback(trace_id: str, feedback: dict):\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0] if runs else None\n",
    "    if run:\n",
    "        client.create_feedback(\n",
    "            key=\"user_rating\",           # a feedback key/tag name\n",
    "            score=feedback[\"rating\"],    # a numeric score (can be 1 if just a comment)\n",
    "            trace_id=trace_id,           # the trace ID to attach feedback to\n",
    "            comment=feedback[\"comment\"]  # the user comment string\n",
    "        )\n",
    "        run_url = client.get_run_url(run=run, project_name=ls_project)\n",
    "        slack_client.chat_postMessage(\n",
    "            channel=channel_id,\n",
    "            text=f\"Feedback received for trace {trace_id}: {feedback['comment']} \\n View trace here: {run_url}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test the function and add some feedback. We'll create a helper printing function to print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(trace_id: str):\n",
    "    trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0]\n",
    "    print(\"Run Input ------------------------------------\")\n",
    "    print(run.inputs[\"question\"] + \"\\n\")\n",
    "    print(\"Run Output -----------------------------------\")\n",
    "    print(run.outputs[\"output\"] + \"\\n\")\n",
    "    print(\"Run Feedback ---------------------------------\")\n",
    "    print(\"Score: \" + str(run.feedback_stats[\"user_rating\"][\"avg\"]))\n",
    "    print(\"Comments: \"+ str(run.feedback_stats[\"user_rating\"][\"comments\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\" # Substitute with your own trace_id\n",
    "feedback = {\"rating\": 4, \"comment\": \"Does not mention the value of LangChain being open source\"}\n",
    "record_user_feedback(trace_id, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Input ------------------------------------\n",
      "Why is LangChain important?\n",
      "\n",
      "Run Output -----------------------------------\n",
      "Okay! Imagine you have a really smart robot that can talk and help you with questions, just like a super nice friend. But sometimes, it can be a bit confusing to tell the robot exactly what you want. \n",
      "\n",
      "LangChain is like a special toy tool that helps people make this robot even better at understanding and answering your questions. It helps your robot learn how to listen better and respond in a way that makes sense. \n",
      "\n",
      "So, if someone wants to create a new game or a fun chat with this robot, LangChain makes it easier for them. It’s like giving them the perfect recipe to make tasty cookies instead of just telling them to bake. This way, they can make their cool robot friend work faster and smarter! \n",
      "\n",
      "That’s why LangChain is important. It helps everyone build better talking robots that can help us in lots of fun and useful ways!\n",
      "\n",
      "Run Feedback ---------------------------------\n",
      "Score: 4.0\n",
      "Comments: ['Does not mention the value of LangChain being open source', 'Does not mention the value of LangChain being open source', 'Does not mention the value of LangChain being open source']\n"
     ]
    }
   ],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "pretty_print(trace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prompt Engineering with the Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, with our feedback recorded, we can find the trace in LangSmith and bring it into the Prompt Playground to reproduce and iterate on it.\n",
    "A link to a sample trace is available here: https://smith.langchain.com/public/2c87b0b2-23f9-4b0f-819e-bdd8a3aec544/r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluating Application Performance with Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluations** are a quantitative way to measure performance of agents, which is important beacause LLMs don't always behave predictably — small changes in prompts, models, or inputs can significantly impact results.\n",
    "\n",
    "Evaluations are made up of three components:\n",
    "\n",
    "1. A **dataset test** inputs and expected outputs.\n",
    "2. An **application or target function** that defines what you are evaluating, taking in inputs and returning the application output\n",
    "3. **Evaluators** that score your target function's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Golden Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to create a golden dataset of prompts we want our application to perform well on. If our application is receiving poor feedback for a given input, it'd be helpful to add that input to our golden dataset. \n",
    "\n",
    "This will allow us to test future versions of our application to make sure it gets the right answer to these problematic inputs!\n",
    "\n",
    "To do this, we do need to record what a good (or \"golden\") response from our application would've looked like. What response would've gotten feedback? We need that golden response so that we have something to measure our application's response against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_golden_dataset(trace_id: str, golden_response: str, dataset_name: str):    \n",
    "    # Fetch the run (trace) by ID\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0] if runs else None\n",
    "    \n",
    "    if run:\n",
    "        if not client.has_dataset(dataset_name=dataset_name):\n",
    "            dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "        else:\n",
    "            dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "        example = {\n",
    "            \"inputs\": run.inputs,\n",
    "            \"outputs\": {\"golden\": golden_response},\n",
    "            \"metadata\": {\"source\": \"trace\"}\n",
    "        }\n",
    "        # Add example to dataset\n",
    "        client.create_examples(dataset_id=dataset.id, examples=[example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either manually generate this golden response using Annotation Queues in LangSmith, or using techniques to generate synthetic data. \n",
    "\n",
    "One potential synthetic approach is to give an LLM the original response and feedback, and have it incorporate the feedback the user provided. Note that synthetic data should still be reviewed by humans before being added to your golden dataset.\n",
    "\n",
    "Let's add a manually created \"golden response\" for our trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "dataset_name = \"eli5-golden-tough\"\n",
    "golden_response = \"\"\"Okay! Imagine you have a really smart robot that can talk and help you with questions, just like a super nice friend. But sometimes, it can be a bit confusing to tell the robot exactly what you want. \n",
    "\n",
    "LangChain is like a special toy tool that helps people make this robot even better at understanding and answering your questions. It helps your robot learn how to listen better and respond in a way that makes sense. \n",
    "\n",
    "So, if someone wants to create a new game or a fun chat with this robot, LangChain makes it easier for them. It’s like giving them the perfect recipe to make tasty cookies instead of just telling them to bake. This way, they can make their cool robot friend work faster and smarter! \n",
    "\n",
    "The best part is that anyone who finds a new recipe can add it to LangChain to make the tool even better! That’s why LangChain is important. It helps everyone build better talking robots that can help us in lots of fun and useful ways!\"\"\"\n",
    "\n",
    "add_to_golden_dataset(trace_id, golden_response, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define an LLM that will compare our application's answer to the golden response in our dataset. We'll tell it to judge based on accuracy. A sample prompt is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for accuracy. Your task is to assign a 0 or 1 based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        An accurate answer:\n",
    "        - Provides accurate information\n",
    "        - Contains no factual errors\n",
    "        - Is complete and not missing any significant information\n",
    "    </Rubric>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a prompt, we can define an evaluator function that actually sends our application's response and the dataset example to an LLM. We've created an accuracy function that does this and returns the result to be recorded in LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a scoring schema that our LLM must adhere to\n",
    "class AccuracyScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "    score: int = Field(description=\"The score of the correctness of the answer, from 0 to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = llm_judge_prompt.format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"golden\"])\n",
    "    structured_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0).with_structured_output(AccuracyScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function that will run our application on the input format provided by our LangSmith dataest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/education/eval-concepts/basics/eli5.py:13: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=1)\n"
     ]
    }
   ],
   "source": [
    "from eli5 import eli5\n",
    "\n",
    "def run(inputs: dict):\n",
    "    return eli5(inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're ready to run an experiment! We've wrapped our evaluate() call in a reporting function so we can send completed experiment results to Slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/education/eval-concepts/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'eli5-gpt-3.5-ff8fde6f' at:\n",
      "https://smith.langchain.com/o/bcad64b4-50f8-4e66-a0be-8dbaf6f6619c/datasets/32cce1c0-b6fe-4be8-9f72-7784fcbf14b5/compare?selectedSessions=b0af9d85-7a1b-453f-99b8-d16d6f3b1826\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:27,  4.65s/it]\n",
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_94660/3649825416.py:14: UserWarning: Function get_test_results is in beta.\n",
      "  df = client.get_test_results(project_name=results.experiment_name)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langsmith import evaluate\n",
    "\n",
    "\n",
    "def eval_and_report(run_func, dataset_name, evaluators, experiment_prefix):\n",
    "    results = evaluate(\n",
    "        run_func,\n",
    "        data=dataset_name,\n",
    "        evaluators=evaluators,\n",
    "        experiment_prefix=experiment_prefix\n",
    "    )\n",
    "    results.wait()\n",
    "    time.sleep(1) # experiment results process asynchronously and return when outputs exist so feedback may still be getting uploaded\n",
    "    df = client.get_test_results(project_name=results.experiment_name)\n",
    "    avg_score = df['feedback.accuracy'].mean()\n",
    "    slack_client.chat_postMessage(\n",
    "        channel=channel_id,\n",
    "        text=f\"Experiment {results.experiment_name} results: {avg_score} accuracy.\"\n",
    "    )\n",
    "    \n",
    "eval_and_report(run, \"eli5-golden-tough\", [accuracy], \"eli5-gpt-3.5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
