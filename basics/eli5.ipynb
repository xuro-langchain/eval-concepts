{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](../images/architecture.png)\n",
    "\n",
    "This example walks through a basic structure you could use to accomplish the following flow:\n",
    "1. Internal & external user feedback is provided in the app for all AI interactions with optional text feedback. We would encourage internal users not to use Slack for this type of feedback.\n",
    "2. This feedback is sent to a Slack channel that we monitor with appropriate repro and debugging information.\n",
    "3. We can easily replay the LLM call\n",
    "Replaying the LLM request uses the same inputs, prompt, model provider, and model name, but these dimensions can be changed and experimented with.\n",
    "4. We iterate on the LLM call until the LLM consistently responds with the expected result. We use LLM-as-a-judge to solidify this expectation and prevent regressions.\n",
    "5. This LLM request replay and LLM-as-a-judge system can be added to our eval test suite, so that it is run alongside our other evals.\n",
    "Evals run post-merge, and results are reported to Slack & internal analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our environment variables from our .env file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)\n",
    "# Loads the following env variables\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5\"\n",
    "# LANGSMITH_API_KEY=\"<redacted>\"\n",
    "# OPENAI_API_KEY=\"<redacted>\"\n",
    "# TAVILY_API_KEY=\"<redacted>\"\n",
    "\n",
    "# SLACK_TOKEN=\"<redacted>\"\n",
    "# SLACK_CHANNEL=\"<redacted>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up our LangSmith client to use the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "ls_project = os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set up a Slack client to send messages programmatically to Slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from slack_sdk import WebClient\n",
    "\n",
    "slack_token = os.getenv(\"SLACK_TOKEN\")\n",
    "channel_id = os.getenv(\"SLACK_CHANNEL\")\n",
    "\n",
    "slack_client = WebClient(token=slack_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a dummy function that represents the function / endpoint your app will call when a user provides feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_user_feedback(trace_id: str, feedback: dict):\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0] if runs else None\n",
    "    if run:\n",
    "        client.create_feedback(\n",
    "            key=\"user_rating\",           # a feedback key/tag name\n",
    "            score=feedback[\"rating\"],    # a numeric score (can be 1 if just a comment)\n",
    "            trace_id=trace_id,           # the trace ID to attach feedback to\n",
    "            comment=feedback[\"comment\"]  # the user comment string\n",
    "        )\n",
    "        run_url = client.get_run_url(run=run, project_name=ls_project)\n",
    "        slack_client.chat_postMessage(\n",
    "            channel=channel_id,\n",
    "            text=f\"Feedback received for trace {trace_id}: {feedback['comment']} \\n View trace here: {run_url}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's test the function and add some feedback. We'll create a helper printing function to print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(trace_id: str):\n",
    "    trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0]\n",
    "    print(\"Run Input ------------------------------------\")\n",
    "    print(run.inputs[\"question\"] + \"\\n\")\n",
    "    print(\"Run Output -----------------------------------\")\n",
    "    print(run.outputs[\"output\"] + \"\\n\")\n",
    "    print(\"Run Feedback ---------------------------------\")\n",
    "    print(\"Score: \" + str(run.feedback_stats[\"user_rating\"][\"avg\"]))\n",
    "    print(\"Comments: \"+ str(run.feedback_stats[\"user_rating\"][\"comments\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "feedback = {\"rating\": 4, \"comment\": \"Does not mention the value of LangChain being open source\"}\n",
    "record_user_feedback(trace_id, feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Input ------------------------------------\n",
      "Why is LangChain important?\n",
      "\n",
      "Run Output -----------------------------------\n",
      "Okay! Imagine you have a really smart robot that can talk and help you with questions, just like a super nice friend. But sometimes, it can be a bit confusing to tell the robot exactly what you want. \n",
      "\n",
      "LangChain is like a special toy tool that helps people make this robot even better at understanding and answering your questions. It helps your robot learn how to listen better and respond in a way that makes sense. \n",
      "\n",
      "So, if someone wants to create a new game or a fun chat with this robot, LangChain makes it easier for them. It’s like giving them the perfect recipe to make tasty cookies instead of just telling them to bake. This way, they can make their cool robot friend work faster and smarter! \n",
      "\n",
      "That’s why LangChain is important. It helps everyone build better talking robots that can help us in lots of fun and useful ways!\n",
      "\n",
      "Run Feedback ---------------------------------\n",
      "Score: 4.0\n",
      "Comments: ['Does not mention the value of LangChain being open source', 'Does not mention the value of LangChain being open source']\n"
     ]
    }
   ],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "pretty_print(trace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, with our feedback recorded, we can find the trace in LangSmith and bring it into the Prompt Playground to reproduce and iterate on it.\n",
    "A link to a sample trace is available here: https://smith.langchain.com/public/2c87b0b2-23f9-4b0f-819e-bdd8a3aec544/r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Application Performance with Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Golden Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to create a golden dataset of prompts we want our application to perform well on. If our application is receiving poor feedback for a given input, it'd be helpful to add that input to our golden dataset. This will allow us to test future versions of our application to make sure it gets the right answer to these problematic inputs!\n",
    "\n",
    "To do this, we do need to record what a good (or \"golden\") response from our application would've looked like. What response would've gotten feedback? We need that golden response so that we have something to measure our application's response against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_golden_dataset(trace_id: str, golden_response: str, dataset_name: str):    \n",
    "    # Fetch the run (trace) by ID\n",
    "    runs = list(client.list_runs(run_ids=[trace_id]))\n",
    "    run = runs[0] if runs else None\n",
    "    \n",
    "    if run:\n",
    "        if not client.has_dataset(dataset_name=dataset_name):\n",
    "            dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "        else:\n",
    "            dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "\n",
    "        example = {\n",
    "            \"inputs\": run.inputs,\n",
    "            \"outputs\": {\"golden\": golden_response},\n",
    "            \"metadata\": {\"source\": \"trace\"}\n",
    "        }\n",
    "        # Add example to dataset\n",
    "        client.create_examples(dataset_id=dataset.id, examples=[example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either manually generate this golden response using Annotation Queues in LangSmith, or using techniques to generate synthetic data. One potential synthetic approach is to give an LLM the original response and feedback, and have it incorporate the feedback the user provided. Note that synthetic data should still be reviewed by humans before being added to your golden dataset.\n",
    "\n",
    "Let's add a manually created \"golden response\" for our trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = \"e97f7a67-5e89-48b3-a54d-bac17411bd6e\"\n",
    "dataset_name = \"eli5-golden-tough\"\n",
    "golden_response = \"\"\"Okay! Imagine you have a really smart robot that can talk and help you with questions, just like a super nice friend. But sometimes, it can be a bit confusing to tell the robot exactly what you want. \n",
    "\n",
    "LangChain is like a special toy tool that helps people make this robot even better at understanding and answering your questions. It helps your robot learn how to listen better and respond in a way that makes sense. \n",
    "\n",
    "So, if someone wants to create a new game or a fun chat with this robot, LangChain makes it easier for them. It’s like giving them the perfect recipe to make tasty cookies instead of just telling them to bake. This way, they can make their cool robot friend work faster and smarter! \n",
    "\n",
    "The best part is that anyone who finds a new recipe can add it to LangChain to make the tool even better! That’s why LangChain is important. It helps everyone build better talking robots that can help us in lots of fun and useful ways!\"\"\"\n",
    "\n",
    "add_to_golden_dataset(trace_id, golden_response, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define an LLM that will compare our application's answer to the golden response in our dataset. We'll tell it to judge based on accuracy. A sample prompt is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for accuracy. Your task is to assign a 0 or 1 based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        An accurate answer:\n",
    "        - Provides accurate information\n",
    "        - Contains no factual errors\n",
    "        - Is complete and not missing any significant information\n",
    "    </Rubric>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a prompt, we can define an evaluator function that actually sends our application's response and the dataset example to an LLM. We've created an accuracy function that does this and returns the result to be recorded in LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a scoring schema that our LLM must adhere to\n",
    "class AccuracyScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "    score: int = Field(description=\"The score of the correctness of the answer, from 0 to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = llm_judge_prompt.format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"golden\"])\n",
    "    structured_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0).with_structured_output(AccuracyScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function that will run our application on the input format provided by our LangSmith dataest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/customer/dbt/eli5.py:13: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=1)\n"
     ]
    }
   ],
   "source": [
    "from eli5 import eli5\n",
    "\n",
    "def run(inputs: dict):\n",
    "    return eli5(inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're ready to run an experiment! We've included a wrapper function to send the results to slack when they're ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/customer/dbt/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'eli5-gpt-3.5-0beadf0f' at:\n",
      "https://smith.langchain.com/o/bcad64b4-50f8-4e66-a0be-8dbaf6f6619c/datasets/32cce1c0-b6fe-4be8-9f72-7784fcbf14b5/compare?selectedSessions=994357a9-fc38-4ec1-a1b6-002d47d3edca\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langsmith import evaluate\n",
    "\n",
    "\n",
    "def eval_and_report(run_func, dataset_name, evaluators, experiment_prefix):\n",
    "    results = evaluate(\n",
    "        run_func,\n",
    "        data=dataset_name,\n",
    "        evaluators=evaluators,\n",
    "        experiment_prefix=experiment_prefix\n",
    "    )\n",
    "    results.wait()\n",
    "    time.sleep(1) # experiment results process asynchronously and return when outputs exist so feedback may still be getting uploaded\n",
    "    df = client.get_test_results(project_name=results.experiment_name)\n",
    "    avg_score = df['feedback.accuracy'].mean()\n",
    "    slack_client.chat_postMessage(\n",
    "        channel=channel_id,\n",
    "        text=f\"Experiment {results.experiment_name} results: {avg_score} accuracy.\"\n",
    "    )\n",
    "    \n",
    "eval_and_report(run, \"eli5-golden-tough\", [accuracy], \"eli5-gpt-3.5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
